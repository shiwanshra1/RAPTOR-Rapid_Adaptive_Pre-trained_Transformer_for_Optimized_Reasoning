
# README: Instruction Finetuning for LLMs

This README covers the process of instruction finetuning for large language models (LLMs) using the Llama 3 model, specifically focusing on evaluation using LitGPT and benchmarking techniques. It is divided into four main sections that guide you through the process, from training to evaluation.

---

## Part 1: Instruction Finetuning Introduction

### Overview

Instruction finetuning is a method to fine-tune a pre-trained LLM to follow specific instructions more effectively. By finetuning, the model learns to generate more accurate and contextually relevant responses to the given tasks. In this part, you will:

- Understand the significance of instruction finetuning.
- Familiarize yourself with the Llama 3 model and LitGPT for evaluation purposes.

---

## Part 2: Generating and Saving Test Set Model Responses

### Objective

In this part, we generate and save the test set responses generated by both the base and finetuned models. The goal is to capture how well the models perform on unseen data by comparing their responses to the expected outputs.

### Steps

1. **Generate Model Responses**: For each test set entry, generate the model's response and save it in the dataset.
2. **Add to Test Set**: Modify the test dataset by adding the model's response alongside the expected output.
3. **Save the Updated Dataset**: The updated dataset is saved as `test_base_model.json` containing both the base model and finetuned model responses for later evaluation.

---

## Part 3: Benchmark Evaluation

### Objective

This section focuses on benchmarking the performance of the finetuned model using MMLU-style Q&A, LLM-based automatic scoring, and human ratings.

### Steps

1. **Set Up Evaluation**: We use the EleutherAI LM Evaluation Harness to evaluate the model using specific MMLU subsets.
2. **Execute Benchmark**: Run the evaluation on the `mmlu_philosophy` subset to test the model's performance on that particular topic.

The evaluation provides quantitative results, helping to compare the finetuned model with the baseline model on a specific task.

---

## Part 4: Evaluating Instruction Responses Locally Using Llama 3 Model

### Objective

This part guides you on how to evaluate the responses generated by the finetuned model using LitGPT with an 8 billion parameter Llama 3 model. The evaluation process compares the responses from the finetuned and non-finetuned models based on a dataset with responses before and after finetuning.

### Steps

1. **Load JSON Dataset**: We load a dataset in JSON format containing both the input, output, and model responses before and after finetuning.
2. **Format for Visualization**: A utility function is used to format the dataset for better readability during evaluation.
3. **Generate Model Scores**: Using LitGPT, the models' responses are scored based on their accuracy and alignment with the expected output. Each response is scored on a scale from 0 to 100.
4. **Evaluate and Compare**: The scores for both the finetuned and non-finetuned models are calculated, and the average score for each model is computed to assess their relative performance.

---

### Final Notes

By following these four parts, you can effectively train, evaluate, and compare instruction finetuned models. Each part focuses on a crucial step in the process: from generating responses to evaluating them against benchmarks and real-world data. The goal is to ensure the finetuned model performs well in following specific instructions and tasks.

---

## File Structure

```
.
├── part1_instruction_finetuning.py
├── part2_generate_test_responses.py
├── part3_benchmark_evaluation.py
├── part4_local_evaluation.py
├── test_base_model.json
└── README.md
```
